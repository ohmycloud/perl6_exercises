
I wanted to try reading a large file using a buffer, and then processing its 
records asynchronously.


Output - to thread or not to thread? {#{{{

    The following shows that if the "process_and_create_output" sub takes .01 
    second per rec, concurrency doesn't help.  But if that process sub becomes 
    expensive enough to take .1 second per rec, the concurrency does help.

    simple processor
        5000 line file with a .01 second sleep:
            No concurrencty:        13 seconds
            with race, 64 batch:    13 seconds
            with race, 32 batch:    13 seconds
            with race, 16 batch:    13 seconds
            with race,  8 batch:    13 seconds
            with race,  4 batch:    13 seconds
            with race,  2 batch:    13 seconds
            with race,  1 batch:    13 seconds

        1000 line file with a .1 second sleep:
            with race, 64 batch:    6.4 seconds
            with race, 32 batch:    3.2 seconds
            with race, 16 batch:    3.2 seconds
            with race,  8 batch:    2.8 seconds
            with race,  4 batch:    2.8 seconds
            with race,  2 batch:    2.6 seconds
            with race,  1 batch:    2.5 seconds


}#}}}
scripts {

    make_file.p6
        Creates "data.txt", which the scripts below are all using as their 
        input file.
        Modify $max to change the size of the file.

    five_process.pl
        Perl5.  Does the same job everything else in here does, using p5 code 
        as I'd write it IRL (which means using Text::CSV_XS).

        This note compares the PP version to the XS version.  There's only one 
        p5 script.  You just need to edit it slightly to switch between 
        versions.
            To use the XS version:
                05 use Text::CSV;
                .. ...
                10 my $csv = Text::CSV->new({ eol => "\012" });

            To use the PP version:
                05 use Text::CSV_PP;
                .. ...
                10 my $csv = Text::CSV_PP->new({ eol => "\012" });

        XS:
            10,000 line file - 0.51 seconds.
        PP:
            10,000 line file - 4.12 seconds.


        If you're wanting to compare the speed of p6 versus p5, the _PP 
        version of this script shows that p6 is slower, but not hugely slower.
                    p5 only:        4.12 seconds
                    p6 fastest:     6.8 seconds

        That's great and all, but IRL I would absolutely not use the _PP 
        version of Text::CSV, I'd use the _XS version.  So for right now, p6 
        isn't even in the same ballpark as p5 for this sort of thing.

        However, comparing the p6 to the p5 with XS isn't totally fair to p6 
        if you're just talking about how fast p5 is versus how fast p6 is.

        If a p6 version of Text::CSV appears that uses NativeCall to call a c 
        library the way the p5 XS version does, the p6 code may well be back 
        to being close.
            I found this: https://sourceforge.net/projects/libcsv/?source=typ_redirect
            I installed it at home with no problem.  It produced 
            /usr/local/lib/libcsv.so (along with some other libcsv* files in 
            that same directory).  Haven't played with it yet.
            I unzipped it into ./libcsv-3.0.3/.  Install is just
                $ ./configure && make && make check
                $ sudo make install
                Huh.  Text::CSV::LibCSV is a thing.  It's on cpan.
            I have more notes in ROOT/modules/NativeCall/README

        BUT, while we're "being fair", remember that the fastest p6 version 
        listed is using multiple threads for output, while the p5 version is a 
        simple single-threaded affair.  The p6 simple_process using no 
        concurrency takes 12.9 seconds, which is 3 times slower than the PP p5 
        version (and around 26 times slower than the XS version).

        Plus, none of the p6 versions are using a CSV module at all, just 
        split() and join().

    simple_process.p6
        Standard stuff.  while(in) { split, process, join, print to out; }.

        The record processing routine is taking the input (1..100 in csv) and 
        running fizzbuzz on each num, then re-CSVing the result.
            10,000 line file, 16.9 seconds

        Originally, my record processing routine was just sleeping for .01 
        seconds and then printing out the record.  These counts are for that:
            With no concurrency, this takes (line_cnt * sleep_time) seconds (derp).
            With concurrent processing on a 5000 line file, it's 13 seconds, regardless of the batch size on the loop.
            With concurrent processing on a 100_000 line file, it's 256 seconds. 

    buffer_process.p6
        Uses a FileBuffer class to read records.  Reads chunks of lines from 
        the file and returns the chunks.
        Those records are then processed using a concurrent for loop.  

        This was a step on the way to channel_process.p6, but is slower and 
        doesn't really stand up on its own so I stopped messing with it.

    channel_process.p6
        Uses a modified version of the FileBuffer class from buffer_process.p6 
        to read records and then write them to a channel.

        Multiple consumers then read those records from the channel and 
        process them.  The multiple consumers are handled via Promises.

        The record processing routine is taking the input (1..100 in csv) and 
        running fizzbuzz on each num, then re-CSVing the result.
            10,000 line file, chunksize 1000, 6.8 seconds

        Originally, my record processing routine was just sleeping for .01 
        seconds and then printing out the record.  These counts are for that:
            For a 5000 line file with a chunksize of 1000, takes 3.5 seconds.
            With a 100_000 line file, chunksize 1000, it's 73.65 seconds (3.5 times faster than the simple process).

        As-is, this program works and is by far the fastest of these scripts.  
        But it's not without its funk.  See the "channel_process.p6 errors" 
        section below, and the errors/ directory which contains some scripts 
        that demonstrate the problem.

    error/
        This contains some scripts meant to show the error channel_process.p6 
        is encountering in as little code as possible.

}


channel_process.p6 errors {#{{{

    This script has two main moving parts.



    The Reader opens the input file and reads lines from it, then prints those 
    lines to a Channel.

    If you look inside the Reader code (the FileBuffer class, get() method), 
    you'll see 3 chunks of code doing the same thing, 2 of which will be 
    commented out.  All 3 of those code chunks work.  I was playing with the 
    three to see which one was more efficient, but stopped messing with that 
    when I realized there were problems with the Writer code. 

    The point is that the Reader is not a problem, even though it contains a 
    decent amount of commented-out code.



    The Writer code consists of a process_records() sub, which opens an output 
    file, then begins reading lines from the Channel, and prints those lines 
    to its output file.

    I'm creating a bunch of calls to that process_records() sub using 
    Promises.  Those sub callse are each passed a unique Int so that the name 
    of the output file they open will be unique across all of the Promises.  
    The idea is to join those files together later.

    The chunk of code causing the problems is pretty simple:
                my $fb  = FileBuffer.new( :file('data.txt'), :chunksize(1000) );
                $fb.get;                        # calls the Reader code
                my $max_writers = 40;
                my @promises;
                for 1..$max_writers -> $n {
                    @promises.push( start {process_records($fb, $n)} );
                }
                await @promises;

        Errors that indicate a line number point at the "await @promises" 
        line.

    There are two different Bad behaviors coming out of that.  As far as I can 
    tell, both appear to be bugs in either Rakudo or MoarVM.

    BAD THING 1: {#{{{
        Reproducing:
            This does not always happen.  If you set up the Conditions as 
            listed and don't get the specified error, just try again.  It 
            seems to happen at least 50% of the time, so you shouldn't have to 
            repeat this too many times to get the error.

        Conditions:
            - $max_writers is set to a lower number.  Start with 20.
            - The output directory is empty, so when the calls to 
              process_records() open their files for write, those files do not 
              already exist.

        Error displayed:
                Earlier failures:
                Failed to open file /home/jon/work/rakudo/projects/process_file_in_chunks/out_channel/foo_7.txt: no such file or directory
                in any  at /home/jon/.rakudo/share/perl6/runtime/CORE.setting.moarvm line 1
                in sub process_records at ./channel_process.p6 line 95
                in block  at ./channel_process.p6 line 86

                Final error:
                Cannot call say(Failure: Str); none of these signatures match:
                    (Mu $: *%_)
                in block <unit> at ./channel_process.p6 line 89

            That error obviously makes no sense.  The full path in the error, 
            out to out_channel/, certainly does exist and is wx, which is 
            demonstrated by the fact that most of the calls to 
            process_records() _were_ able to create their files.  Just ls 
            out_channel after an error run to see that there are output files 
            in there.

        Solution II (bad):
            Use a non-concurrent sub to make sure that all of the output files 
            actually exist before calling a bunch of process_records() 
            Promises.
                for 1..$max_writers -> $n {
                    my $fn = "out_channel/{$n}.txt";
                    shell( "touch $fn" );
                }

            If you run the script as it currently exists (without that 
            non-concurrent file-creation sub above) multiple times in a row, 
            without clearing out out_channel/ in between, you'll eventually 
            get to the point where all of the output files exist in 
            out_channel/.  After that happens, subsequent runs work just fine.

            Adding that non-concurrent sub to create the empty files would fix 
            this problem.  It'd also be a Horrible Workaround that should not 
            need to be in there.

    }#}}}
    BAD THING 2: {#{{{
        Conditions:
            - $max_writers is set to a higher number.  Start with 80.
            - This error happens most of the time even when the output 
              directory already has files in it, but appears to happen every 
              time when it doesn't, so empty out out_channel/ first.

        Error displayed:
                fish: Job 1, “./channel_process.p6 ” terminated by signal SIGABRT (Abort)

            One time (but only once), I got two lines of error text:
                fish: Job 1, “./channel_process.p6 ” terminated by signal SIGABRT (Abort)
                double free or corruption (fasttop)

            The "double free..." error is not verbatim.  I googled for it, 
            only using part of the error text.  Now that I want to copy the 
            full error text into here, it's out of my scrollback.

        Solution:
            Use a smaller $max_writers value.  That's it.  I don't know what's 
            actually causing this or how to reasonably fix it other than 
            reducing $max_writers.

    }#}}}

}#}}}




